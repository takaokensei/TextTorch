{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TextTorch - 06: Deploy e Inferência**\n",
    "\n",
    "**Objetivo:** Demonstrar como usar o modelo treinado para fazer predições em novos textos. Este notebook simula um ambiente de \"deploy\" onde carregamos os artefatos salvos (modelo e vectorizer) para inferência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Como Executar (Google Colab)**\n",
    "\n",
    "1. **Pré-requisito:** Execute os notebooks `01` a `05` para gerar todos os artefatos.\n",
    "2. **Ambiente:** Se estiver em um novo ambiente, clone o repositório e instale as dependências.\n",
    "   ```bash\n",
    "   !git clone https://github.com/takaokensei/TextTorch.git\n",
    "   %cd TextTorch\n",
    "   !pip install -r requirements.txt\n",
    "   ```\n",
    "3. **Execução:** Execute todas as células."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 17:32:37,077 - INFO - Configuração carregada de: ../models/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente configurado.\n"
     ]
    }
   ],
   "source": [
    "# Imports e Configurações Iniciais\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diretório 'src' ao path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from deploy import Predictor, save_inference_example\n",
    "from model import load_config\n",
    "\n",
    "# Carrega configurações\n",
    "CONFIG_PATH = '../models/config.yaml'\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "print(\"Ambiente configurado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregando o Preditor\n",
    "\n",
    "A classe `Predictor` do módulo `src/deploy.py` encapsula toda a lógica necessária para a inferência:\n",
    "- Carrega o modelo treinado.\n",
    "- Carrega o `TfidfVectorizer`.\n",
    "- Preprocessa o texto de entrada.\n",
    "- Realiza a predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 17:32:40,562 - INFO - Inicializando preditor...\n",
      "2025-11-13 17:32:40,570 - INFO - TFIDFClassifier criado: input=676, hidden=512, output=6\n",
      "2025-11-13 17:32:40,572 - INFO - Modelo carregado de: ../models/tfidf_model.pt\n",
      "2025-11-13 17:32:40,581 - INFO - Preditor inicializado com sucesso.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preditor carregado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Caminhos para os artefatos\n",
    "MODEL_PATH = f\"../models/{config['representation']}_model.pt\"\n",
    "VECTORIZER_PATH = '../artifacts/vectorizer.joblib'\n",
    "\n",
    "# Instancia o preditor\n",
    "try:\n",
    "    predictor = Predictor(model_path=MODEL_PATH, vectorizer_path=VECTORIZER_PATH)\n",
    "    print(\"Preditor carregado com sucesso!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERRO: {e}\")\n",
    "    print(\"\\nCertifique-se de que os notebooks anteriores foram executados e os artefatos foram salvos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Teste Interativo de Predição\n",
    "\n",
    "Agora, podemos usar o objeto `predictor` para classificar novos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: 'The new graphics card from Nvidia offers amazing performance...'\n",
      "  -> Predição: Classe 1 (Confiança: 48.73%)\n",
      "\n",
      "Texto: 'Scientists are debating the existence of dark matter in the ...'\n",
      "  -> Predição: Classe 1 (Confiança: 19.43%)\n",
      "\n",
      "Texto: 'O governo anunciou novas medidas para controlar a inflação....'\n",
      "  -> Predição: Classe 1 (Confiança: 23.51%)\n",
      "\n",
      "Texto: 'This email is an urgent security alert for your account....'\n",
      "  -> Predição: Classe 1 (Confiança: 19.43%)\n",
      "\n",
      "Texto: 'How can I fix this error in my Python code?...'\n",
      "  -> Predição: Classe 1 (Confiança: 19.43%)\n",
      "\n",
      "Texto: 'The best hockey players will compete for the Stanley Cup....'\n",
      "  -> Predição: Classe 1 (Confiança: 19.43%)\n",
      "\n",
      "Texto: 'A discussão sobre a reforma tributária continua no congresso...'\n",
      "  -> Predição: Classe 4 (Confiança: 26.01%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de textos para testar\n",
    "textos_exemplo = [\n",
    "    \"The new graphics card from Nvidia offers amazing performance for gaming.\", # Computação/Gráficos\n",
    "    \"Scientists are debating the existence of dark matter in the universe.\", # Ciência/Espaço\n",
    "    \"O governo anunciou novas medidas para controlar a inflação.\", # Política/Economia (em português)\n",
    "    \"This email is an urgent security alert for your account.\", # Spam/Alerta\n",
    "    \"How can I fix this error in my Python code?\", # Programação\n",
    "    \"The best hockey players will compete for the Stanley Cup.\", # Esportes\n",
    "    \"A discussão sobre a reforma tributária continua no congresso.\", # Política (em português)\n",
    "]\n",
    "\n",
    "for texto in textos_exemplo:\n",
    "    try:\n",
    "        label, prob = predictor.predict(texto)\n",
    "        print(f\"Texto: '{texto[:60]}...'\\n  -> Predição: {label} (Confiança: {prob:.2%})\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao predizer o texto: '{texto[:60]}...'. Causa: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Salvando Exemplo de Inferência\n",
    "\n",
    "Para documentar o projeto, salvamos um arquivo de texto com os resultados das predições de exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 17:33:02,895 - INFO - Exemplo de inferência salvo em: ../reports/example_inference.txt\n"
     ]
    }
   ],
   "source": [
    "INFERENCE_EXAMPLE_PATH = '../reports/example_inference.txt'\n",
    "\n",
    "try:\n",
    "    save_inference_example(predictor, textos_exemplo, save_path=INFERENCE_EXAMPLE_PATH)\n",
    "except NameError:\n",
    "    print(\"Preditor não foi carregado. Pulando salvamento de exemplo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
